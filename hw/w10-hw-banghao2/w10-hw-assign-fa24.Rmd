---
title: "Week 10 - Homework"
author: "STAT 420, Fall 2024, Banghao Chi"
date: '11/03/2024'
output:
  pdf_document: default
  html_document: 
    theme: readable
    toc: yes
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.

```{r}
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", 
                       alpha = 0.05, plotit = TRUE, testit = TRUE) {
  resids = residuals(model)
  fitted_vals = fitted(model)
  
  if(testit) {
    sw_test = shapiro.test(resids)
    output = list(
      p_val = sw_test$p.value,
      decision = ifelse(sw_test$p.value < alpha, "Reject", "Fail to Reject")
    )
  }
  
  if(plotit) {
    par(mfrow = c(1, 2))
    
    plot(fitted_vals, resids, 
         col = pcol,
         xlab = "Fitted",
         ylab = "Residuals",
         main = "Residuals vs Fitted Values")
    abline(h = 0, col = lcol, lwd = 2)
    
    qqnorm(resids, 
           col = pcol,
           main = "Normal Q-Q Plot")
    qqline(resids, col = lcol, lwd = 2)
  }
  
  if(testit) {
    return(output)
  }
}
```

**(b)** Run the following code.

```{r}
set.seed(40)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```

```{r}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```

***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.

```{r}
model = lm(lpsa ~ ., data = prostate)
r_squared = summary(model)$r.squared
```

The $R^2$ value for this model is `r r_squared`.

**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
plot(model, which = 1)
```

Look at the Fitted vs Residuals plot, I don't think the constant variance assumption for this model is violated, as for any fitted value, the distribution of the residuals seem to be similar (in terms of range and density).

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
plot(model, which = 2)
```

Look at the Normal Q-Q plot, I think the normality assumption for this model is violated, there seems to be fat tails within the plot. But if we check the p-value of the SW test, we can see a large value, where we won't reject the null hypothesis, meaning that the normality assumption is not violated.

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

```{r}
leverage = hatvalues(model)
high_leverage_cutoff = 2 * mean(leverage)
high_leverage = which(leverage > high_leverage_cutoff)
prostate[high_leverage, ]
```

The above is the reported observations that have high leverage.

**(e)** Check for any influential observations. Report any observations you determine to be influential.

```{r}
cd = cooks.distance(model)
n = length(cd)
influential_cutoff = 4/n
influential_points = which(cd > influential_cutoff)
prostate[influential_points, ]
```

The above is the reported observations that are influential.

**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
model_cleaned = lm(lpsa ~ ., data = prostate[-influential_points,])

coef_comparison = data.frame(
    Original = coef(model),
    Without_Influential = coef(model_cleaned),
    Difference = coef(model_cleaned) - coef(model)
)
round(coef_comparison, 4)
```

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
removed_data = prostate[influential_points,]
pred_original = predict(model, newdata = removed_data)
pred_cleaned = predict(model_cleaned, newdata = removed_data)

prediction_comparison = data.frame(
    Observation = influential_points,
    Original_Pred = pred_original,
    Clean_Model_Pred = pred_cleaned,
    Difference = pred_cleaned - pred_original
)
round(prediction_comparison, 4)
```

When we remove influential points and refit the model, our predictions for those influential points change noticeably. This change in predictions demonstrates how these points were indeed influential - removing them affected the model's parameters enough to create different predictions.

***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter esimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(83)
library(lmtest)
y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
bptest(fit_1)
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(83)
y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
bptest(fit_2)
```

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 20021023
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

```{r}
for(i in 1:num_sims) {
  x_1 = runif(n, 0, 5)
  x_2 = runif(n, -2, 2)
  
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
  
  fit_1 = lm(y_1 ~ x_1 + x_2)
  fit_2 = lm(y_2 ~ x_1 + x_2)
  
  p_val_1[i] = summary(fit_1)$coefficients[3,4]
  p_val_2[i] = summary(fit_2)$coefficients[3,4]
}
```

**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r}
prop_01_1 = mean(p_val_1 < 0.01)
prop_01_2 = mean(p_val_2 < 0.01)

prop_05_1 = mean(p_val_1 < 0.05)
prop_05_2 = mean(p_val_2 < 0.05)

prop_10_1 = mean(p_val_1 < 0.10)
prop_10_2 = mean(p_val_2 < 0.10)

results = data.frame(
  Alpha = c(0.01, 0.05, 0.10),
  Model1 = c(prop_01_1, prop_05_1, prop_10_1),
  Model2 = c(prop_01_2, prop_05_2, prop_10_2)
)
results
```

The proportion that we see in the above table is rates of the probability of rejecting $H_0: \beta_2 = 0$ when it is actually true. For model 1, the rates should be close to the alpha level (which is exactly the case as we see in above table), since it's variance of the noise is indeed a constant; for model 2, since the noise is not a constant, the rates should slightly deviate from the value of alpha due to the violation of the homoscedasticity assumption.

***

## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

```{r}
model1 = lm(loss ~ Fe, data=corrosion)

plot(loss ~ Fe, data=corrosion, 
     main="Loss vs Iron Content",
     xlab="Iron Content (Fe)", 
     ylab="Loss")
abline(model1, col="darkorange", lwd = 2, lty = 2)
```

```{r}
par(mfrow=c(1,2))
plot(model1, which=1)
plot(model1, which=2)

bptest(model1)
shapiro.test(resid(model1))
```

We check the assumption using statistical method and also **art** (visual) method.

- **Statistically**:
  - For BP test, since the p-value of the test is $0.9$, which is greater than most of the alpha value, we fail to reject the null hypothesis. This suggests that the assumption of constant variance is satisfied.
  - For SW test, since the p-value of the test is $0.4$, which is greater than most of the alpha value, we fail to reject the null hypothesis. This suggests that the assumption of residuals are normally distributed.

- **Visually**:
  - For Fitted vs Residuals plot, we see that the distribution of the residuals vary according to the value of the fitted values, which suggests that the equal variance assumption is violated.
  - For the Nornal Q-Q plot, we can see a slight fat tail on the right of the plot, which I suspect the assumption of normality.
  
**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

```{r}
model2 = lm(loss ~ Fe + I(Fe^2), data=corrosion)
model3 = lm(loss ~ Fe + I(Fe^2) + I(Fe^3), data=corrosion)
model4 = lm(loss ~ Fe + I(Fe^2) + I(Fe^3) + I(Fe^4), data=corrosion)

par(mfrow=c(1,3))

plot(fitted(model2), resid(model2),
     main="Quadratic Model",
     xlab="Fitted values",
     ylab="Residuals")
abline(h=0, col="darkorange", lty=2)

plot(fitted(model3), resid(model3),
     main="Cubic Model",
     xlab="Fitted values",
     ylab="Residuals")
abline(h=0, col="darkorange", lty=2)

plot(fitted(model4), resid(model4),
     main="Quartic Model",
     xlab="Fitted values",
     ylab="Residuals")
abline(h=0, col="darkorange", lty=2)

anova(model2, model3, model4)

shapiro.test(resid(model3))

cd = cooks.distance(model3)
cd_high_cutoff = 4 / length(cd)
index = which(cd > cd_high_cutoff)
corrosion[index, ]
```

- Based on the plots, the Cubic model seems to be the best in my opinion, as the distribution of the residuals seem to be scattered equally no matter the value of fitter value is.
- Based on anova table:
  - Comparing model2 (quadratic) to model3 (cubic):
    - The p-value is 0.0075 (< 0.05), indicating that adding the cubic term significantly improves the model
    - The RSS decreases substantially from 100.1 to 45.1
  - Comparing model3 (cubic) to model4 (quartic):
    - The p-value is 0.1675 (> 0.05), indicating that adding the quartic term does not significantly improve the model
    - The RSS only decreases slightly from 45.1 to 35.0
  - Therefore, Cubic model is the best choice according to he anova table.
- The p-value of the SW test for this model(Cubic Model) is 0.9, which is greater than most of the common alpha values. Therefore, the normality assumption for this model is satisfied.
- There's no influential points based on the threshold of $\frac{4}{n}$.

***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
```

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

```{r}
model_linear = lm(price ~ carat, data = diamonds)
summary(model_linear)
```

**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 

```{r}
plot(diamonds$carat, diamonds$price,
     pch = 20,
     xlab = "Carat",
     ylab = "Price",
     main = "Price vs Carat with Linear Fit")

abline(model_linear, col = "darkorange", lwd = 2, lty = 2)
par(mfrow=c(1,2))
plot(model_linear, which=1)
plot(model_linear, which=2)
```

- For linearity: According to the scattered plot, we can see that the relationship is something other than linear, as the regression doesn't fit so well.
- For equal variance assumption: The distribution of the residuals seem to have a relationship with fitted value, therefore the variance of the distribution of the noise may not be equal.
- For normality assumption: Flat tails are observed within the Normal Q-Q plot, suggesting that the noise may not follow a normal distribution.

**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
qplot(price, data = diamonds, bins = 30)
model_log_y = lm(log(price) ~ carat, data = diamonds)

plot(diamonds$carat, log(diamonds$price),
     pch = 20,
     xlab = "Carat",
     ylab = "Log(Price)",
     main = "Log(Price) vs Carat with Linear Fit")

abline(model_log_y, col = "darkorange", lwd = 2, lty = 2)


par(mfrow=c(1,2))
plot(model_log_y, which=1)
plot(model_log_y, which=2)
```

- For linearity: According to the scattered plot, we can see that the relationship is something other than linear, as the regression doesn't fit so well.
- For equal variance assumption: The distribution of the residuals seem to have a relationship with fitted value, therefore the variance of the distribution of the noise may not be equal.
- For normality assumption: Flat tail is observed within the Normal Q-Q plot, suggesting that the noise may not follow a normal distribution.

**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
model_log_both = lm(log(price) ~ log(carat), data = diamonds)

plot(log(diamonds$carat), log(diamonds$price),
     pch = 20,
     xlab = "Log(Carat)",
     ylab = "Log(Price)",
     main = "Log(Price) vs Log(Carat) with Linear Fit")

abline(model_log_both, col = "darkorange", lwd = 2, lty = 2)

par(mfrow=c(1,2))
plot(model_log_both, which=1)
plot(model_log_both, which=2)
```

- For linearity: According to the scattered plot, we can see that the relationship is indeed linear, as the regression fits well.
- For equal variance assumption: The distribution of the residuals seem to have a relationship with fitted value, therefore the variance of the distribution of the noise may not be equal.
- For normality assumption: Flat tail is observed within the Normal Q-Q plot, suggesting that the noise may not follow a normal distribution.

**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).

```{r}
new_data = data.frame(carat = 3)
log_pred = predict(model_log_both, 
                   newdata = data.frame(carat = log(3)), 
                   interval = "prediction", 
                   level = 0.99)
exp(log_pred)
```

